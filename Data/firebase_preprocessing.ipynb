{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GYAty_ammuTr"
      ],
      "authorship_tag": "ABX9TyNf61kB+O26uQfVsZ2GUGXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sensify-Lab/Community-Comm/blob/main/Data/firebase_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Community Comm\n",
        "\n",
        "### Firebase Preprocessing\n"
      ],
      "metadata": {
        "id": "eHmqtMOFmc6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the menu: 'Runtime >> Run All' to run all cells in order"
      ],
      "metadata": {
        "id": "AmjvKjp1Ih2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Setup"
      ],
      "metadata": {
        "id": "GYAty_ammuTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import files\n",
        "from datetime import date\n",
        "from scipy import stats\n",
        "\n",
        "'''\n",
        "# for pulling data from google drive:\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials \n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "z5yq0jUyLWXs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "88a86050-d677-4726-bd59-8cb7d832db18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# for pulling data from google drive:\\n\\nfrom pydrive.auth import GoogleAuth\\nfrom pydrive.drive import GoogleDrive\\nfrom google.colab import auth\\nfrom oauth2client.client import GoogleCredentials \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "YpgCAgi7I5xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# id='1qum959PUDAuevThqVhVjLykU1lcqUNKD'\n",
        "# downloaded = drive.CreateFile({'id': id}) \n",
        "# downloaded.GetContentFile('user_log_october_22.json')\n",
        "\n",
        "'''\n",
        "\n",
        "Normally, we would read data from the Google Drive, but our json file\n",
        "seems to be too large for this. We'll use files.upload() to grab it\n",
        "from local storage instead.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "uploaded = files.upload() "
      ],
      "metadata": {
        "id": "mG2lnt16Vql1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "3665ec36-26fd-4c28-a0ab-b29c573c734a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cfccea5c-2c51-480e-993c-c79c8a56c35f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cfccea5c-2c51-480e-993c-c79c8a56c35f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving user_log_october_22.json to user_log_october_22.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(uploaded.values()))\n",
        "\n",
        "d = json.loads(data.decode())\n",
        "\n",
        "df = pd.DataFrame(d)"
      ],
      "metadata": {
        "id": "QrK4LQv-MtlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "P6netqSaOEui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_articles(json):\n",
        "\n",
        "  rowData = {\n",
        "          \n",
        "          \"days_active\" : [],\n",
        "          \n",
        "          \"days_with_article\" : [],\n",
        "          \n",
        "          \"total_articles\" : [],\n",
        "      \n",
        "          \"article_rate\" : [],\n",
        "\n",
        "          \"bias\" : [],\n",
        "          \n",
        "          \"left_pct\" : [],\n",
        "\n",
        "          \"left_center_pct\" : [],\n",
        "\n",
        "          \"center_pct\" : [],\n",
        "\n",
        "          \"right_center_pct\" : [],\n",
        "\n",
        "          \"right_pct\" : []\n",
        "      \n",
        "      }\n",
        "\n",
        "  for user in json:\n",
        "      \n",
        "      days = 0\n",
        "      \n",
        "      n_articles = 0\n",
        "      \n",
        "      has_article = 0\n",
        "\n",
        "      bias = []\n",
        "      \n",
        "      #print(user)\n",
        "      \n",
        "      for day in json[user]: \n",
        "      \n",
        "          #print(day)\n",
        "          \n",
        "          days += 1\n",
        "          \n",
        "          if \"articles\" in json[user][day]:\n",
        "\n",
        "              articles_df = pd.DataFrame(json[user][day][\"articles\"])\n",
        "            \n",
        "              has_article += 1\n",
        "              \n",
        "              n_articles += len(json[user][day][\"articles\"])\n",
        "\n",
        "              bias = bias + list(articles_df.loc[\"bias\",:].values)\n",
        "              \n",
        "              \n",
        "\n",
        "      rowData[\"days_active\"].append(days)\n",
        "      rowData[\"days_with_article\"].append(has_article)\n",
        "      rowData[\"total_articles\"].append(n_articles)\n",
        "      rowData[\"article_rate\"].append(has_article/days)\n",
        "      rowData[\"bias\"].append(stats.mode(bias)[0])\n",
        "\n",
        "      if (len(bias)>0):\n",
        "        rowData[\"left_pct\"].append(float(bias.count(\"left\"))/float(len(bias)))\n",
        "        rowData[\"left_center_pct\"].append(float(bias.count(\"left-center\"))/float(len(bias)))\n",
        "        rowData[\"center_pct\"].append(float(bias.count(\"center\"))/float(len(bias)))\n",
        "        rowData[\"right_center_pct\"].append(float(bias.count(\"right-center\"))/float(len(bias)))\n",
        "        rowData[\"right_pct\"].append(float(bias.count(\"right\"))/float(len(bias)))\n",
        "\n",
        "      else:\n",
        "        rowData[\"left_pct\"].append(0)\n",
        "        rowData[\"left_center_pct\"].append(0)\n",
        "        rowData[\"center_pct\"].append(0)\n",
        "        rowData[\"right_center_pct\"].append(0)\n",
        "        rowData[\"right_pct\"].append(0)\n",
        "\n",
        "  # print(rowData)\n",
        "  df = pd.DataFrame(data=rowData, index=json.keys(),columns=[\"days_active\", \"days_with_article\", \"total_articles\",\"article_rate\", \"bias\",\"left_pct\",\"left_center_pct\",\"center_pct\",\"right_center_pct\", \"right_pct\"])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "M5a__KiDN-Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_browsing(json):\n",
        "\n",
        "  rowData = {\n",
        "          \n",
        "          \"total_seconds\" : [],\n",
        "\n",
        "          \"avg_sec_per_day\" : [],\n",
        "\n",
        "          \"median_sec_per_day\" : [],\n",
        "\n",
        "          \"unique_domains\" : []\n",
        "\n",
        "          #\"top_domain\" : []\n",
        "      \n",
        "      }\n",
        "\n",
        "  for user in json:\n",
        "\n",
        "    domain_count = pd.DataFrame()\n",
        "\n",
        "    domains = np.array([], dtype=\"str\")\n",
        "\n",
        "    seconds = np.array([])\n",
        "\n",
        "    for date in json[user]: \n",
        "\n",
        "      if (\"articles\" in json[user][date].keys()): user_day_df = pd.DataFrame(json[user][date]).drop(\"articles\", axis=1)\n",
        "\n",
        "      else: user_day_df = pd.DataFrame(json[user][date])\n",
        "\n",
        "      seconds = np.append(seconds, user_day_df.loc[\"seconds\", :].sum())\n",
        "\n",
        "      domains = np.append(domains, user_day_df.loc[\"website\",:].unique())\n",
        "\n",
        "      domains_df = pd.DataFrame(user_day_df.T)\n",
        "\n",
        "      domains_df = pd.DataFrame(domains_df[domains_df[\"pushType\"]==0].loc[:,\"website\"], columns=[\"website\"])\n",
        "\n",
        "      domains_df[f\"{date}\"] = 1\n",
        "\n",
        "      domains_df = domains_df.groupby(\"website\").count()\n",
        "\n",
        "      if (domain_count.empty): domain_count = domains_df\n",
        "      else: domain_count = domain_count.join(domains_df, \"website\").fillna(0)\n",
        "      # print(domain_count)\n",
        "\n",
        "    rowData[\"total_seconds\"].append(seconds.sum())\n",
        "\n",
        "    rowData[\"avg_sec_per_day\"].append(np.mean(seconds))\n",
        "\n",
        "    rowData[\"median_sec_per_day\"].append(np.median(seconds))\n",
        "\n",
        "    rowData[\"unique_domains\"].append(len(np.unique(domains.astype(\"str\"))))\n",
        "\n",
        "    #rowData[\"top_domain\"].append()\n",
        "\n",
        "    print(seconds)\n",
        "\n",
        "  df = pd.DataFrame(data=rowData, index=json.keys(), columns = [\"total_seconds\", \"avg_sec_per_day\", \"median_sec_per_day\" ,\"unique_domains\"])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "MYuKSxyhGZHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_user_browsing(json, user):\n",
        "\n",
        "  rowData = {\n",
        "          \n",
        "          \"total_seconds\" : [],\n",
        "\n",
        "          \"unique_domains\" : []\n",
        "      }\n",
        "\n",
        "\n",
        "  for date in json[user]: \n",
        "\n",
        "    seconds = 0\n",
        "\n",
        "    domains = 0\n",
        "\n",
        "    user_day_df = pd.DataFrame(json[user][date])\n",
        "\n",
        "    seconds = user_day_df.loc[\"seconds\", :].sum()\n",
        "\n",
        "    domains = user_day_df.loc[\"website\",:].unique().size\n",
        "\n",
        "    rowData[\"total_seconds\"].append(seconds)\n",
        "\n",
        "    rowData[\"unique_domains\"].append(domains)\n",
        "\n",
        "  df = pd.DataFrame(data=rowData, index=json[user].keys(), columns = [\"total_seconds\", \"unique_domains\"])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "uL4Z8y7xAoLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_articles(json):\n",
        "\n",
        "  articles = {}\n",
        "\n",
        "  for user in json:\n",
        "\n",
        "    user_df = pd.DataFrame(json[user])\n",
        "\n",
        "    if \"articles\" in user_df.index:\n",
        "      #print(user_df.index)\n",
        "      for i in user_df.loc[\"articles\", :].dropna():\n",
        "\n",
        "        for k in i.keys():\n",
        "\n",
        "          if k in articles.keys() and user not in articles[k]:\n",
        "\n",
        "            articles[k].append(user)\n",
        "\n",
        "          elif k not in articles.keys():\n",
        "            articles[k] = [user]\n",
        "\n",
        "  return articles"
      ],
      "metadata": {
        "id": "BI6B-kdhAZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feature_table(json, exclude=[]):\n",
        "\n",
        "  filter_accounts(json, exclude) # This function modifies the json argument directly, returns nothing. \n",
        "\n",
        "  browsing_df = describe_browsing(json)\n",
        "\n",
        "  articles_df = describe_articles(json)\n",
        "\n",
        "  table = browsing_df.join(articles_df)\n",
        "\n",
        "  table.index.name = \"user\"\n",
        "\n",
        "  return table\n"
      ],
      "metadata": {
        "id": "MeFPRtIZA5Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_accounts(json, exclude):\n",
        "\n",
        "  # NOTE : MODIFIES THE JSON DIRECTLY, RETURNS NOTHING\n",
        "\n",
        "  for i in exclude:\n",
        "    json.pop(i, None)"
      ],
      "metadata": {
        "id": "nLp6Ss-kBIn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run & Export"
      ],
      "metadata": {
        "id": "1rwsjOI8Ay2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# Keep a list of dev keys to filter out\n",
        "#\n",
        "# This can also be done by passing through 'exclude' as an argument to generate_feature_table()\n",
        "#\n",
        "# Prerana 3uQeHE04bAOUjM8FIGnrCddmL7Y2\n",
        "# Matt 2d4Hsxo6DDZsSgL5XlCRSHnlSRN2\n",
        "# Luke qkE8zvjG9jQrjqLZh6E83qUmxXt2\n",
        "# Nabiha ARMWazZdFHXXJQR46TLSoicFtLv2\n",
        "#\n",
        "\n",
        "exclude = [\"3uQeHE04bAOUjM8FIGnrCddmL7Y2\", \"2d4Hsxo6DDZsSgL5XlCRSHnlSRN2\", \"qkE8zvjG9jQrjqLZh6E83qUmxXt2\", \"ARMWazZdFHXXJQR46TLSoicFtLv2\"]\n",
        "\n",
        "filter_accounts(d, exclude) # This function modifies the json argument directly, returns nothing. (also called inside generate_feature_table)"
      ],
      "metadata": {
        "id": "27DyzwKaBqMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the feature table\n",
        "\n",
        "df = generate_feature_table(d)"
      ],
      "metadata": {
        "id": "pDakp4D-Bjvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_list = collect_articles(d)"
      ],
      "metadata": {
        "id": "F6UNOVXtQXoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_list"
      ],
      "metadata": {
        "id": "PKtn9gJ4a99N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df = pd.DataFrame([articles_list.keys(), articles_list.values()]).T"
      ],
      "metadata": {
        "id": "phY63REkTYT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export feature table to CSV file\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "curr_date = today.strftime(\"%b_%d_%Y\")\n",
        "\n",
        "df.to_csv(f\"feature_table_{curr_date}.csv\") # name the file after today's date\n",
        "\n",
        "files.download(f\"feature_table_{curr_date}.csv\")"
      ],
      "metadata": {
        "id": "QAHU7bXbF8d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export articles df to CSV file\n",
        "\n",
        "articles_df.to_csv(\"articles_list.csv\")\n",
        "\n",
        "files.download(\"articles_list.csv\")"
      ],
      "metadata": {
        "id": "nKYJ3_HMMPof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}